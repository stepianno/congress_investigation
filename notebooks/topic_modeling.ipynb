{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create 15 appropriate topic models for the collection of bills from the past four years that I previously collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "config = {\n",
    "    'host': 'xx.xxx.xxx.xx',\n",
    "    'username': 'xxxxxxxx',\n",
    "    'password': 'xxxxxxxx',\n",
    "    'authSource': 'cool'\n",
    "}\n",
    "\n",
    "client = MongoClient(**config)\n",
    "db = client.cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    '''\n",
    "    This function does the work of preprocessing on all bills by removing unnecessary characters and punctuation\n",
    "    from the text and preparing it for NLP\n",
    "    '''\n",
    "    text = re.sub('\\\\n', ' ', text)\n",
    "    text = re.sub('_', ' ', text)\n",
    "    text = re.sub('\\[.{,50}\\]', ' ', text)\n",
    "    text = re.sub('\\<.{,10}\\>', ' ', text)\n",
    "    text = re.sub('[.\\'`,;():\\-$%&\\^#?!><]', ' ', text).lower()\n",
    "    text = re.sub('\\w*\\d+\\w*', '', text)\n",
    "    text = re.sub('\\s[xvil]+\\s', ' ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aggregated a number of frequently occurring words in the topic modeling process that failed to add any meaning to my topics and I added them to the stop words to later be used in CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stops = ['secretary', 'insert','inserting','striking','program','plan','note','rule','agency',\n",
    "             'administration','subparagraph','later','services','appropriate','administrator',\n",
    "             'commission','faa', 'national','usc','assistance','eligible','service','clause',\n",
    "             'development','respect','apply','case','percent','determined','board','subtitle',\n",
    "             'person','period','office','use','programs','individuals','systems','revies','study',\n",
    "             'public','management','administering','imposed','authority','relating','state',\n",
    "             'federal','project','law','review','central','agencies','subsection','san',\n",
    "             'subsection','activities','attorney','department','resolution','joint','rules',\n",
    "             'date','effect','described','motion','report','major','days','day','consideration',\n",
    "             'order', 'amended','paragraph','entities','counsel','mr','ms','proposed','information',\n",
    "             'local','written','affairs', 'non','year','month','members','committees','government',\n",
    "             'head','member','general','shall','action','assessment','subject','including',\n",
    "             'available','purposes','term','provided','remain','necessary','used','end','president',\n",
    "             'zzz','zz','zuni','zuhair','zte','zour','zou','zoster','zoris','zor', 'heading','subheading',\n",
    "             'chapter','change','new','subchapter','numerical','sequence','designed','appropriated',\n",
    "             'september',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add my stop words to the list of English stop words already used in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stops = text.ENGLISH_STOP_WORDS\n",
    "stops = list(stops) + real_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I iterate through every bill in my database. I add the preprocessed bill to the list 'texts' to be used for topic modeling, while all the other details of the bill are made into a dictionary and appended to the list 'details' to be converted into a DataFrame and used for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = []\n",
    "texts = []\n",
    "cursor = db.bills.find({})\n",
    "for bill in cursor:\n",
    "    texts.append(text_clean(bill['bill']))\n",
    "    bill_dict = {'_id': bill['_id'], 'congress': bill['congress'], 'date': bill['date'],\n",
    "                'track': bill['track'], 'sponsor': bill['name'], 'party': bill['party'],\n",
    "                'state': bill['state']}\n",
    "    details.append(bill_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills = pd.DataFrame(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills.set_index('_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.8,stop_words=stops, token_pattern='[a-z]{4,}', max_features=3000)\n",
    "X = cv.fit_transform(texts)\n",
    "df = pd.DataFrame(X.toarray(), index=df_bills.index, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I opted to use LDA for my topic modelling as these are larger texts. LDA seemed to provide better results than NMF for this data. I also utilize pyLDAviz to help determine the appropriacy of number of topics as well as identify characteristics of each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 15\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components,   \n",
    "                                      max_iter=10,                 \n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,            \n",
    "                                      n_jobs = -1,                \n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, X, cv)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear here that the distribution of topic assignment is reasonable and pyLDAviz demonstrates that there is not too much overlap between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cluster = lda_output.argmax(axis = 1)\n",
    "df_cluster = pd.DataFrame(doc_cluster, index=df_bills.index)\n",
    "df_cluster[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills.to_csv('bill_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I visualize the top 14 words of each topic in order to assign an appropriate name to each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_words = lda_model.components_.argsort(axis=1)[:,-1:-15:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [[df.columns[n] for n in row] for row in model_words]\n",
    "list(enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {0: 'small business',\n",
    "              1: 'health care',\n",
    "              2: 'elections/voting',\n",
    "              3: 'regulations*',\n",
    "              4: 'labor/employee rights',\n",
    "              5: 'border/immigration',\n",
    "              6: 'education',\n",
    "              7: 'law enforcement',\n",
    "              8: 'budgetary',\n",
    "              9: 'military',\n",
    "              10: 'farmaceuticals/medicine',\n",
    "              11: 'Trump response',\n",
    "              12: 'natural resources',\n",
    "              13: 'infrastructure',\n",
    "              14: 'tax code'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then assign every bill its majority topic, add the topic to the bill on the dataframe, and write the dataframe to a csv for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_topic(n):\n",
    "    return topic_dict[n]\n",
    "df_cluster['names'] = df_cluster[0].apply(name_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills['topic'] = df_cluster['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills.to_csv('bill_details.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
